{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi8Tc383HcOv"
   },
   "source": [
    "# 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRNNZxNEHbbf"
   },
   "source": [
    "Identify the differences between Google Patent's architecture and the code given in the module:\n",
    "\n",
    "\n",
    "1.   The design layout given out in Google's patent is for their translation services, so they have used both encoder, decoder networks while we have just used the decoder only architecture.\n",
    "\n",
    "2.   Google's patent has the provision to combine inputs from encoder and decoder into a multi-attention layer, our code avoids this step because of the decoder-only architecture.\n",
    "\n",
    "3. A significant deviation is in the positional encoder where original architecture employs a sinusoidal version,but the code uses learned positional encoders.\n",
    "\n",
    "4. The initial discrepancy I identified lies in the embedding size used within the code, where embed_size is set to 128, whereas the paper specifies d_model as 512. This is modifiable based on the application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQqQxBBaOlWL"
   },
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auO2HrtjOnzs"
   },
   "source": [
    "Modifications on the code:\n",
    "\n",
    "1.   Modified the code by adding code to perform positional encoding using sinusoidal waveform.\n",
    "\n",
    "2.   Also modified the code to run the code on gpu.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Gt5FmglFrSb",
    "outputId": "50c7e128-de3d-4648-88d2-d28ce3bd1c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 1115739\n",
      "iter 0: train 5.704237937927246 val 5.720011234283447\n",
      "iter 1000: train 1.4822652339935303 val 1.6298166513442993\n",
      "iter 2000: train 1.3000602722167969 val 1.5403438806533813\n",
      "iter 3000: train 1.1915180683135986 val 1.528475284576416\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "    # Embed the input ids using the token embeddings table\n",
    "      in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:]).to(self.device)\n",
    "      if self.is_pos_emb:\n",
    "          # Compute the sinusoidal positional embeddings\n",
    "          # Define a helper function inside the forward method\n",
    "          def get_pos_emb(length, d_model, device):\n",
    "              # Create a tensor of positions from 0 to length - 1\n",
    "              pos = torch.arange(length, dtype=torch.float, device=device).unsqueeze(1)\n",
    "              # Create a tensor of scaling factors for each dimension\n",
    "              scale = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).to(self.device)\n",
    "              # Create a zero tensor for the positional embeddings\n",
    "              pe = torch.zeros(length, d_model, device=device)\n",
    "              # Compute the sine and cosine values for the even and odd dimensions\n",
    "              pe[:, 0::2] = torch.sin(pos * scale)\n",
    "              pe[:, 1::2] = torch.cos(pos * scale)\n",
    "              return pe\n",
    "          # Get the positional embeddings for the input ids\n",
    "          pos_emb = get_pos_emb(in_ids[:, -self.input_length:].shape[1], in_ids_emb.shape[-1], self.device)\n",
    "          # Add the positional embeddings to the token embeddings\n",
    "          in_ids_emb = in_ids_emb + pos_emb\n",
    "      # Pass the embedded input through the transformer blocks\n",
    "      block_outputs = self.blocks(in_ids_emb)\n",
    "      # Project the output of the transformer blocks to the vocabulary size\n",
    "      logits = self.linear_sahead_to_vocab(block_outputs)\n",
    "      # Compute the cross-entropy loss if the target is given\n",
    "      if target is not None:\n",
    "          # Reshape the logits and the target to match the expected shape\n",
    "          batch_size, input_length, vocab_size = logits.shape\n",
    "          logits = logits.view(batch_size * input_length, vocab_size)\n",
    "          target = target.view(batch_size * input_length)\n",
    "          # Compute the cross-entropy loss\n",
    "          ce_loss = F.cross_entropy(logits, target)\n",
    "      else:\n",
    "          # Set the loss to None\n",
    "          ce_loss = None\n",
    "      # Return the logits and the loss\n",
    "      return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in range(train_iters):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long).to(self.device)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size).to(self.device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size).to(self.device),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size).to(self.device),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size).to(self.device),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size).to(self.device),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size).to(self.device),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size).to(self.device),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size).to(self.device)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch\n",
    "\n",
    "\n",
    "with open('./WarrenBuffet.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
    "#        'lazy dog and a quick brown fox.\\n' \\\n",
    "#        'the dog is lazy and the fox jumps quickly.\\n' \\\n",
    "#        'a fox jumps over the dog because he is lazy.\\n' \\\n",
    "#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\n",
    "\n",
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model = model.to(model.device)\n",
    "model.prep(text)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "_, _ = model(input_batch, output_batch)\n",
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRNlYkBdtTsI",
    "outputId": "b309ff00-624d-4666-d51f-6ee4b96aa1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "action In for meting $679 million. In 2006, specially public charging \n",
      "the 80-year loan \n",
      "of money. Charlie and I serie about $2 billion. I am eain it head 2005, him \n",
      "commpanies. You can of you same we than told Sedream a CEO of A, \n",
      "\n",
      "Nernigh Ne Marking, Fred, has never become on by its nothing: The Eleven \"Every earnings paragre of this her purchase ackin \n",
      "of every investment for intrancial $100 lets year. At a \n",
      "some probacks for the questions that \n",
      "the most of our customers in the world \n",
      "on Capmarks \n",
      "and Picking it Jack bolt-or \"them.\" \n",
      "\n",
      "112 \n",
      "\n",
      "\n",
      "Desciones all of cters, weak, how many have \n",
      "rolioys. All of their each substantial compilated returned by Justin books. \n",
      "\n",
      "\n",
      "17 \n",
      "\n",
      "\n",
      "BERA period a posses to have a value of combined comities), no low \n",
      "adding simple. Over time, the cost of the other industry. I fest implices and, esce- of since business and losses $650 million of directors in shift take tools I. \n",
      "\n",
      "\n",
      "26 \n",
      "\n",
      "\n",
      "The midame have none just moneting Seath at I decided, the induring of mysself \n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=1000)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQviZdtvTn9X"
   },
   "source": [
    "# 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG5KFvzFTszW"
   },
   "source": [
    "Model performance: Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kfw--uKlQYFO",
    "outputId": "6751717d-844c-4987-eb8b-abb8faa97d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.55\n",
      "Perplexity: 4.69\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def calculate_perplexity(model, eval_data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    data_loader = DataLoader(eval_data, batch_size=model.batch_size, shuffle=False)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(model.device), targets.to(model.device)\n",
    "\n",
    "            logits, _ = model(inputs, targets)  # Feed the inputs to the model\n",
    "            logits = logits.view(-1, model.vocab_size)\n",
    "\n",
    "            # Calculate cross-entropy loss\n",
    "            ce_loss = F.cross_entropy(logits, targets.contiguous().view(-1), reduction='sum')\n",
    "            total_loss += ce_loss.item()\n",
    "\n",
    "            # Count the total number of tokens in the evaluation data\n",
    "            total_tokens += targets.numel()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    print(f\"Average Loss: {avg_loss:.2f}\")\n",
    "\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))  # Calculate perplexity using the average loss\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "# Example usage:\n",
    "eval_data = model.get_batch(split=\"eval\")\n",
    "perplexity = calculate_perplexity(model, eval_data)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ue071WfWSqU"
   },
   "source": [
    "Average Loss: 1.55\n",
    "The average loss is a measure of how well the model is predicting the target sequence. Lower values indicate better performance. In this case, an average loss of 1.55 is relatively low, suggesting that, on average, the model is making accurate predictions.\n",
    "\n",
    "Perplexity: 4.69\n",
    "Perplexity is another way to measure the quality of a language model. It is a measure of how well the model predicts the next token in a sequence. Lower perplexity values indicate better performance. A perplexity of 4.69 is quite good and suggests that the model is generally effective at predicting the next token in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX5o1pfSUKjP"
   },
   "source": [
    "# 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ_77m6VUO64"
   },
   "source": [
    "Most impressive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5eG_ruzFbtU",
    "outputId": "f01c3b12-ea20-4c43-e7d9-dda5b6839063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highlight of the year, however, was our July 5 th acquisition of most of ISCAR, an Israeli \n",
      "company, and our new association with its chairman, Eitan Wertheimer, and CEO, Jacob Harpaz. And ISncor', \n",
      "\n",
      "Abounts claims produce of Ederior in 1798, we well need Leceivable Berkshire. This losses \n",
      "being slow wish perform, ployit deevelopied into the great authout 50% over at the bligared. They did of the old because of a suming you surly produce years when the past: market it somehodd, seven \n",
      "challege addvertisting mansude wondership or many self-salent in the future recommed of a policy recoverine to come from from more than $200 $times. \n",
      "cash equivalent of completing to us that helple: And achieved, in a fline funding its this told you wisdom exposed us when their products after someone \n",
      "doil services. Because off financie \n",
      "housing \n",
      "commitments to ustriste \n",
      "we agned to on depread pay in a \n",
      "medical from Blumkin  share wither sume operation. No in the higgh-yieldings, that are exords. Overage the borne \n",
      "Marmon partnelled Pocific, however, there is happer in bought North Clayton Home of Fruic operations, \n",
      "the most holding trouble earned 9/30; can we unipos say, the housing \n"
     ]
    }
   ],
   "source": [
    "seed_phrase = \"\"\"The highlight of the year, however, was our July 5 th acquisition of most of ISCAR, an Israeli\n",
    "company, and our new association with its chairman, Eitan Wertheimer, and CEO, Jacob Harpaz\"\"\"\n",
    "context_token_ids = torch.tensor([model.encoder(seed_phrase)], dtype=torch.long).to(model.device)\n",
    "\n",
    "outputs = model.generate(context_token_ids=context_token_ids, max_new_tokens=1000)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT9JslPRZYBj"
   },
   "source": [
    "Even though the model is not up to the mark and there are a few grammatical, and spelling mistakes it can provide relevant text. Especially in this case when I fed the text about acquisition, it was able to look into the data for the data about acquisitions and provided us with information about other acquisitions.\n",
    "The most impactful design choices responsible for the text according to me are multi-head attention layers that we have developed, using which the text is able to relate and produce meaningful results by understanding context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KQW0fP9FZaZ"
   },
   "source": [
    "Multi-Head Self-Attention:\n",
    "\n",
    "The model uses multi-head self-attention to capture dependencies between different positions in the input sequence. This allows the model to focus on different parts of the input when making predictions for each position.\n",
    "\n",
    "Positional Embeddings:\n",
    "The model incorporates positional embeddings to give the model information about the order of tokens in the input sequence. This is crucial for understanding the sequential nature of language.\n",
    "\n",
    "Feedforward Neural Network (MLP):\n",
    "The model includes a feedforward neural network (MLP) within each Transformer block. This adds a non-linear layer after the self-attention mechanism, allowing the model to learn complex relationships and representations.\n",
    "\n",
    "Layer Normalization:\n",
    "Layer normalization is applied in various parts of the model, helping to stabilize training and improve convergence. It normalizes the activations, making them more robust during training.\n",
    "\n",
    "Training and Evaluation Methods:\n",
    "The model implements a training loop with Adam optimizer and cross-entropy loss. It includes methods for both training and evaluating the model. The training loop prints out performance metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5arXerEwA2O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
